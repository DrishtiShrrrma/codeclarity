[
  {
    "code": "def add_netnode_plugin_name(plugin_name):\n    \"\"\"\n    Add the given plugin name to the list of plugin names registered in\n      the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"\n    current_names = set(get_netnode_plugin_names())\n    if plugin_name in current_names:\n        return\n\n    current_names.add(plugin_name)\n\n    get_meta_netnode()[PLUGIN_NAMES_KEY] = json.dumps(list(current_names))",
    "docstring": "Add the given plugin name to the list of plugin names registered in\n      the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "The function `add_netnode_plugin_name` adds a given plugin name to the list of registered plugin names in the current IDB. It takes one argument, `plugin_name`, which is a string representing the name of the plugin to be added. The function first retrieves the current set of plugin names using `get_netnode_plugin_names()`. It then checks if the provided `plugin_name` is already in this set. If it is, the function returns immediately without making any changes. Otherwise, it adds the `plugin_name` to the set, converts the updated set back to a list, serializes it to a JSON string, and stores it in the meta netnode under the key `PLUGIN_NAMES_KEY`. This ensures the new plugin name is registered in the IDB.",
    "summary_chinese": "函数名为 `add_netnode_plugin_name`，用于将给定的插件名称添加到当前 IDB 中已注册的插件名称列表中。该函数接受一个参数 `plugin_name`，类型为字符串。函数的主要逻辑是：首先获取当前已注册的插件名称集合，检查待添加的插件名称是否已存在，如果不存在则将其添加到集合中，最后将更新后的插件名称列表以 JSON 格式存储到元数据网络节点中。",
    "summary_french": "La fonction `add_netnode_plugin_name` permet d'ajouter un nom de plugin à la liste des plugins enregistrés dans la base de données IDB actuelle. Elle prend un seul argument, `plugin_name`, qui est une chaîne de caractères représentant le nom du plugin à ajouter. La fonction commence par récupérer l'ensemble des noms de plugins déjà enregistrés via `get_netnode_plugin_names`. Si le nom du plugin est déjà présent dans cet ensemble, la fonction se termine sans effectuer d'action supplémentaire. Sinon, elle ajoute le nom du plugin à l'ensemble, puis met à jour la métadonnée associée dans la base de données IDB en utilisant `get_meta_netnode` et en stockant la liste des noms de plugins sous forme de chaîne JSON.",
    "summary_spanish": "Nombre de la función: add_netnode_plugin_name\n\nDescripción: Esta función agrega un nombre de plugin a la lista de plugins registrados en la base de datos IDB actual.\n\nArgumentos:\n- plugin_name (str): El nombre del plugin que se desea agregar.\n\nLógica clave: La función primero obtiene los nombres de plugins actualmente registrados y los almacena en un conjunto. Luego, verifica si el nombre del plugin ya existe en este conjunto. Si no existe, lo agrega al conjunto y actualiza la lista de plugins en la base de datos IDB, convirtiendo el conjunto de nuevo a una lista y almacenándolo como una cadena JSON.",
    "summary_portuguese": "Nome da função: add_netnode_plugin_name\n\nDescrição: Adiciona um nome de plugin à lista de plugins registrados no IDB atual.\n\nArgumentos:\n- plugin_name (str): O nome do plugin a ser adicionado.\n\nLógica principal: A função primeiro obtém os nomes dos plugins atualmente registrados e verifica se o novo nome já está presente. Se não estiver, adiciona o novo nome à lista, converte a lista atualizada para JSON e armazena-a de volta no IDB.",
    "summary_arabic": "اسم الدالة: add_netnode_plugin_name\n\nوصف الغرض: تضيف هذه الدالة اسم إضافة (plugin) إلى قائمة أسماء الإضافات المسجلة في قاعدة البيانات الحالية (IDB).\n\nالحجج وأنواعها:\n- plugin_name (سلسلة نصية): اسم الإضافة التي سيتم إضافتها.\n\nملخص المنطق الرئيسي:\n1. يتم الحصول على مجموعة أسماء الإضافات الحالية من قاعدة البيانات.\n2. يتم التحقق مما إذا كان اسم الإضافة موجودًا بالفعل في المجموعة. إذا كان موجودًا، تنتهي الدالة دون إجراء أي تغيير.\n3. إذا لم يكن الاسم موجودًا، يتم إضافته إلى المجموعة.\n4. يتم تحديث قاعدة البيانات بحفظ قائمة أسماء الإضافات المحدثة في شكل JSON.",
    "summary_hindi": "फ़ंक्शन का नाम: add_netnode_plugin_name\n\nउद्देश्य: यह फ़ंक्शन वर्तमान IDB में पंजीकृत प्लगइन नामों की सूची में एक नया प्लगइन नाम जोड़ता है।\n\nआर्ग्युमेंट:\n- plugin_name (स्ट्रिंग): जोड़ने के लिए प्लगइन का नाम।\n\nमुख्य तर्क:\nफ़ंक्शन पहले वर्तमान में पंजीकृत प्लगइन नामों की सूची प्राप्त करता है और उन्हें एक सेट में संग्रहीत करता है। यदि दिया गया प्लगइन नाम पहले से ही सेट में मौजूद है, तो फ़ंक्शन कुछ नहीं करता और वापस आ जाता है। अन्यथा, यह नया प्लगइन नाम सेट में जोड़ता है और अपडेट की गई सूची को JSON फॉर्मेट में परिवर्तित करके मेटा नेटनोड में संग्रहीत करता है।"
  },
  {
    "code": "def _wire_kernel(self):\n        \"\"\"Initializes the kernel inside GTK.\n        \n        This is meant to run only once at startup, so it does its job and\n        returns False to ensure it doesn't get run again by GTK.\n        \"\"\"\n        self.gtk_main, self.gtk_main_quit = self._hijack_gtk()\n        gobject.timeout_add(int(1000*self.kernel._poll_interval),\n                            self.iterate_kernel)\n        return False",
    "docstring": "Initializes the kernel inside GTK.\n        \n        This is meant to run only once at startup, so it does its job and\n        returns False to ensure it doesn't get run again by GTK.",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "The function `_wire_kernel` initializes the kernel inside GTK. It is designed to run only once at startup. The function takes no arguments. Its key logic involves hijacking GTK's main loop using the `_hijack_gtk` method, storing the results in `self.gtk_main` and `self.gtk_main_quit`. It then adds a timeout callback using `gobject.timeout_add` to periodically call `self.iterate_kernel` at intervals determined by `self.kernel._poll_interval`. Finally, it returns `False` to prevent GTK from running this function again.",
    "summary_chinese": "函数名为 `_wire_kernel`，其目的是初始化 GTK 内部的 kernel。该函数设计为仅在启动时运行一次，执行完任务后返回 `False` 以确保 GTK 不会再次调用它。函数接受一个参数 `self`，表示类的实例。其核心逻辑包括：劫持 GTK 的 `gtk_main` 和 `gtk_main_quit` 方法，并使用 `gobject.timeout_add` 添加一个定时器，定时调用 `self.iterate_kernel` 方法，间隔时间为 `self.kernel._poll_interval` 秒。",
    "summary_french": "La fonction `_wire_kernel` initialise le noyau (kernel) dans l'environnement GTK. Elle est conçue pour être exécutée une seule fois au démarrage. Elle récupère les fonctions `gtk_main` et `gtk_main_quit` via la méthode `_hijack_gtk`, puis configure un timer avec `gobject.timeout_add` pour appeler la méthode `iterate_kernel` à intervalles réguliers, déterminés par `self.kernel._poll_interval`. Enfin, elle retourne `False` pour s'assurer qu'elle ne soit pas réexécutée par GTK. \n\nArguments : Aucun (méthode d'instance). \n\nLogique clé : Initialisation unique du noyau GTK avec un timer pour des itérations périodiques.",
    "summary_spanish": "El nombre de la función es _wire_kernel y su propósito es inicializar el kernel dentro de GTK. Esta función está diseñada para ejecutarse solo una vez al inicio. Sus argumentos son: self, que es una instancia de la clase que contiene este método. La lógica clave de la función es: primero, se llama al método _hijack_gtk para obtener las funciones gtk_main y gtk_main_quit, que se almacenan en los atributos self.gtk_main y self.gtk_main_quit. Luego, se utiliza gobject.timeout_add para programar la ejecución periódica del método iterate_kernel con un intervalo determinado por self.kernel._poll_interval. Finalmente, la función devuelve False para asegurarse de que no se ejecute nuevamente por GTK.",
    "summary_portuguese": "Nome da função: _wire_kernel\n\nDescrição: Inicializa o kernel dentro do GTK. Esta função é projetada para ser executada apenas uma vez na inicialização, realizando sua tarefa e retornando False para garantir que não seja executada novamente pelo GTK.\n\nArgumentos: A função não recebe argumentos explícitos, mas utiliza atributos da instância (self), como self.kernel e self.gtk_main.\n\nLógica principal: A função primeiro chama o método _hijack_gtk() para configurar as variáveis self.gtk_main e self.gtk_main_quit. Em seguida, utiliza gobject.timeout_add para agendar a execução periódica do método iterate_kernel, com um intervalo baseado no atributo self.kernel._poll_interval. Por fim, retorna False para evitar que o GTK a execute novamente.",
    "summary_arabic": "اسم الدالة: _wire_kernel\n\nوصف الغرض: تقوم هذه الدالة بتهيئة النواة داخل GTK. يتم تصميمها لتعمل مرة واحدة فقط عند بدء التشغيل، حيث تقوم بعملها ثم تعيد False لضمان عدم تشغيلها مرة أخرى بواسطة GTK.\n\nالحجج وأنواعها:\n- self: كائن الفئة الحالية (object)\n\nالمنطق الرئيسي:\n1. تقوم الدالة باستدعاء الدالة _hijack_gtk() لتهيئة gtk_main و gtk_main_quit.\n2. تستخدم الدالة gobject.timeout_add لإضافة دالة iterate_kernel كدالة مؤقتة يتم تشغيلها بشكل دوري كل فترة زمنية محددة بواسطة self.kernel._poll_interval.\n3. تعيد الدالة False لضمان عدم تشغيلها مرة أخرى بواسطة GTK.",
    "summary_hindi": "फ़ंक्शन का नाम: `_wire_kernel`\n\nउद्देश्य: यह फ़ंक्शन GTK के अंदर केर्नल को शुरू करता है। यह केवल स्टार्टअप पर एक बार चलने के लिए डिज़ाइन किया गया है।\n\nआर्ग्युमेंट्स: इस फ़ंक्शन के पास कोई सीधा आर्ग्युमेंट नहीं है, लेकिन यह `self` के माध्यम से क्लास के इंस्टेंस वेरिएबल्स (`gtk_main`, `gtk_main_quit`, `kernel`, `_poll_interval`) का उपयोग करता है।\n\nमुख्य तर्क: यह फ़ंक्शन पहले `_hijack_gtk` मेथड को कॉल करके `gtk_main` और `gtk_main_quit` को इनिशियलाइज़ करता है। उसके बाद, यह `gobject.timeout_add` का उपयोग करके `iterate_kernel` मेथड को एक निर्दिष्ट इंटरवल (`_poll_interval`) पर बार-बार कॉल करने के लिए सेट करता है। अंत में, यह `False` रिटर्न करता है ताकि GTK इसे फिर से न चलाए।"
  },
  {
    "code": "def substring_after(ctx, full, part):\n    '''\n    Yields one string\n    '''\n    full = next(string_arg(ctx, full), '')\n    part = next(string_arg(ctx, part), '')\n    yield full.partition(part)[-1]",
    "docstring": "Yields one string",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "The function `substring_after` extracts and returns the portion of a string that comes after a specified substring. It takes three arguments: `ctx` (context), `full` (the full string to search within), and `part` (the substring to search for). Both `full` and `part` are expected to be string arguments, with `ctx` likely being a context object used to retrieve these strings. The function uses Python's `partition` method to split the `full` string at the first occurrence of `part`, and then yields the part of the string that comes after the split. If `part` is not found in `full`, the entire `full` string is returned. The function handles cases where `full` or `part` might not be provided by defaulting them to an empty string.",
    "summary_chinese": "函数名为 `substring_after`，其目的是返回一个字符串，该字符串是 `full` 字符串中 `part` 之后的部分。函数接受三个参数：`ctx`（上下文对象）、`full`（完整字符串，字符串类型）和 `part`（部分字符串，字符串类型）。其核心逻辑是首先获取 `full` 和 `part` 的字符串值，然后使用 `partition` 方法将 `full` 字符串分割成三部分，最后返回分割后的第三部分，即 `part` 之后的内容。",
    "summary_french": "La fonction `substring_after` a pour but de retourner la partie d'une chaîne de caractères qui suit une sous-chaîne spécifiée. Elle prend trois arguments : `ctx` (contexte), `full` (chaîne complète, de type chaîne de caractères) et `part` (sous-chaîne, de type chaîne de caractères). La fonction utilise `string_arg` pour récupérer les valeurs de `full` et `part`, avec des valeurs par défaut vides si elles ne sont pas fournies. Ensuite, elle utilise la méthode `partition` pour diviser la chaîne `full` en trois parties autour de la sous-chaîne `part`, et retourne la dernière partie, qui correspond à la sous-chaîne située après `part`.",
    "summary_spanish": "La función se llama `substring_after` y su propósito es devolver la subcadena que sigue a la primera aparición de una cadena específica dentro de otra cadena. Sus argumentos son `ctx` (contexto), `full` (cadena completa, tipo string) y `part` (subcadena a buscar, tipo string). La lógica principal consiste en utilizar el método `partition` para dividir la cadena completa en tres partes basadas en la subcadena buscada, y luego devolver la tercera parte, que es la subcadena que sigue a la primera aparición de `part` en `full`. Si `part` no se encuentra en `full`, se devuelve una cadena vacía.",
    "summary_portuguese": "Nome da função: substring_after\n\nDescrição: Retorna uma substring que vem após a primeira ocorrência de uma parte especificada em uma string completa.\n\nArgumentos:\n- ctx (contexto): Contexto de execução, não é diretamente utilizado na lógica principal.\n- full (string): A string completa na qual a busca será realizada.\n- part (string): A parte da string que será procurada dentro da string completa.\n\nLógica principal: A função primeiro obtém os valores de 'full' e 'part' como strings, utilizando a função 'string_arg' e o método 'next' para garantir que sejam tratados como strings. Em seguida, utiliza o método 'partition' para dividir a string 'full' na primeira ocorrência de 'part', e retorna a última parte do resultado, que corresponde à substring após a ocorrência de 'part'.",
    "summary_arabic": "اسم الدالة: substring_after\n\nوصف الغرض: تقوم هذه الدالة بإرجاع الجزء من السلسلة النصية الكاملة الذي يأتي بعد الجزء المحدد.\n\nالحجج وأنواعها:\n- ctx: سياق أو بيئة التنفيذ (غير محدد النوع في الكود المعطى)\n- full: السلسلة النصية الكاملة (نوعها سلسلة نصية)\n- part: الجزء المحدد الذي سيتم البحث عنه (نوعها سلسلة نصية)\n\nملخص المنطق الرئيسي:\nتقوم الدالة أولاً بتحويل الحجج full و part إلى سلاسل نصية باستخدام الدالة string_arg، ثم تستخدم الدالة partition لتقسيم السلسلة النصية الكاملة إلى ثلاثة أجزاء: الجزء قبل الجزء المحدد، والجزء المحدد نفسه، والجزء بعد الجزء المحدد. يتم إرجاع الجزء الأخير (الجزء بعد الجزء المحدد) باستخدام الترميز [-1].",
    "summary_hindi": "फ़ंक्शन का नाम: substring_after\n\nउद्देश्य: यह फ़ंक्शन एक स्ट्रिंग के भीतर दूसरी स्ट्रिंग के बाद का हिस्सा लौटाता है।\n\nआर्ग्युमेंट्स:\n- ctx: कंटेक्स्ट (संभवतः कॉलिंग फ़ंक्शन का कंटेक्स्ट)\n- full: मुख्य स्ट्रिंग (स्ट्रिंग)\n- part: वह स्ट्रिंग जिसके बाद का हिस्सा निकालना है (स्ट्रिंग)\n\nमुख्य तर्क:\nफ़ंक्शन पहले 'full' और 'part' आर्ग्युमेंट्स को स्ट्रिंग में बदलता है, और फिर 'full' स्ट्रिंग को 'part' स्ट्रिंग के आधार पर तीन हिस्सों में बाँटता है। अंत में, यह 'part' के बाद का हिस्सा (तीसरा हिस्सा) लौटाता है। यदि 'part' 'full' में नहीं मिलता है, तो पूरी 'full' स्ट्रिंग लौटाई जाती है।"
  },
  {
    "code": "def _insert_html(self, cursor, html):\n        \"\"\" Inserts HTML using the specified cursor in such a way that future\n            formatting is unaffected.\n        \"\"\"\n        cursor.beginEditBlock()\n        cursor.insertHtml(html)\n\n        # After inserting HTML, the text document \"remembers\" it's in \"html\n        # mode\", which means that subsequent calls adding plain text will result\n        # in unwanted formatting, lost tab characters, etc. The following code\n        # hacks around this behavior, which I consider to be a bug in Qt, by\n        # (crudely) resetting the document's style state.\n        cursor.movePosition(QtGui.QTextCursor.Left,\n                            QtGui.QTextCursor.KeepAnchor)\n        if cursor.selection().toPlainText() == ' ':\n            cursor.removeSelectedText()\n        else:\n            cursor.movePosition(QtGui.QTextCursor.Right)\n        cursor.insertText(' ', QtGui.QTextCharFormat())\n        cursor.endEditBlock()",
    "docstring": "Inserts HTML using the specified cursor in such a way that future\n            formatting is unaffected.",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "The function `_insert_html` is a method designed to insert HTML content into a document using a specified cursor while ensuring that future text formatting remains unaffected. It takes three arguments: `self` (the instance of the class), `cursor` (a `QTextCursor` object representing the insertion point), and `html` (a string containing the HTML content to be inserted). The function begins by starting an edit block with `cursor.beginEditBlock()` and inserts the HTML using `cursor.insertHtml(html)`. To prevent unwanted formatting issues caused by the document remaining in \"HTML mode\" after insertion, the function performs a series of steps: it moves the cursor left while keeping the anchor, checks if the selected text is a space, and removes it if so. If not, it moves the cursor right, inserts a plain space with no formatting, and ends the edit block with `cursor.endEditBlock()`. This process ensures that subsequent text additions are treated as plain text, avoiding formatting inconsistencies.",
    "summary_chinese": "函数名：_insert_html\n\n用途：使用指定的光标插入HTML，确保后续格式不受影响。\n\n参数：\n- self：对象实例（通常用于类方法）\n- cursor：光标对象，用于在文本文档中定位和操作\n- html：要插入的HTML字符串\n\n关键逻辑：\n1. 开始编辑块，确保操作原子性。\n2. 插入HTML内容。\n3. 处理Qt的“HTML模式”问题：移动光标到左侧，检查是否有空格，如果有则删除；否则移动光标到右侧。\n4. 插入一个空格并重置样式状态，以避免后续文本格式异常。\n5. 结束编辑块，提交更改。",
    "summary_french": "La fonction `_insert_html` insère du HTML à l'emplacement spécifié par un curseur tout en évitant les problèmes de formatage futurs. Elle prend deux arguments : `cursor` (un objet QTextCursor) et `html` (une chaîne de caractères contenant le HTML à insérer). La logique principale consiste à démarrer un bloc d'édition, insérer le HTML, puis réinitialiser l'état de style du document pour éviter les problèmes de formatage indésirables. Pour ce faire, elle déplace le curseur à gauche, vérifie si un espace a été ajouté, le supprime si nécessaire, puis insère un espace avec un format de caractère vide avant de terminer le bloc d'édition.",
    "summary_spanish": "Nombre de la función: _insert_html\n\nDescripción: Inserta HTML en un documento de texto utilizando un cursor especificado, asegurando que el formato futuro no se vea afectado.\n\nArgumentos:\n- self (instancia de la clase)\n- cursor (QtGui.QTextCursor): el cursor que indica la posición de inserción.\n- html (str): el código HTML a insertar.\n\nLógica clave: La función comienza un bloque de edición en el cursor, inserta el HTML y luego realiza una serie de operaciones para evitar problemas de formato. Mueve el cursor a la izquierda, verifica si hay un espacio en blanco seleccionado y lo elimina si es necesario. Si no hay espacio, mueve el cursor a la derecha. Luego, inserta un espacio en blanco con un formato de carácter vacío para reiniciar el estado de estilo del documento. Finalmente, termina el bloque de edición. Estas operaciones evitan que el documento \"recuerde\" que está en modo HTML, lo que podría causar problemas de formato en inserciones posteriores de texto plano.",
    "summary_portuguese": "Nome da função: _insert_html\n\nDescrição: Insere HTML em um documento de texto usando um cursor especificado, garantindo que a formatação futura não seja afetada.\n\nArgumentos:\n- self (instância da classe)\n- cursor (QtGui.QTextCursor): cursor usado para inserir o HTML\n- html (str): conteúdo HTML a ser inserido\n\nLógica principal: A função começa um bloco de edição no cursor e insere o HTML fornecido. Em seguida, ela realiza uma série de operações para \"resetar\" o estado de estilo do documento, evitando que a inserção de texto simples posterior seja afetada pela formatação HTML. Isso é feito movendo o cursor para a esquerda, verificando se há um espaço em branco selecionado e removendo-o, ou movendo o cursor para a direita e inserindo um espaço com formatação padrão. Por fim, a função encerra o bloco de edição no cursor.",
    "summary_arabic": "اسم الدالة: _insert_html\n\nالغرض: إدراج HTML باستخدام المؤشر المحدد بطريقة لا تؤثر على التنسيق المستقبلي.\n\nالحجج:\n- self: كائن الذات (object)\n- cursor: مؤشر النص (QTextCursor)\n- html: النص HTML المراد إدراجه (str)\n\nالمنطق الرئيسي:\nتبدأ الدالة ببدء كتلة تحرير باستخدام المؤشر، ثم تدرج النص HTML المحدد. بعد الإدراج، يتم التعامل مع مشكلة تذكر المستند أنه في \"وضع HTML\"، مما قد يؤدي إلى تنسيق غير مرغوب فيه عند إضافة نص عادي لاحقًا. يتم حل هذه المشكلة عن طريق تحريك المؤشر إلى اليسار، وإزالة أي مسافة زائدة، ثم تحريك المؤشر إلى اليمين وإدراج مسافة فارغة بتنسيق نصي عادي. أخيرًا، تنتهي كتلة التحرير.",
    "summary_hindi": "फ़ंक्शन का नाम: `_insert_html`\n\nउद्देश्य: यह फ़ंक्शन किसी निर्दिष्ट कर्सर का उपयोग करके HTML को ऐसे सम्मिलित करता है कि भविष्य का फॉर्मेटिंग प्रभावित न हो।\n\nआर्ग्युमेंट्स:\n- `self`: ऑब्जेक्ट का संदर्भ (संभवतः क्लास का हिस्सा)\n- `cursor`: टेक्स्ट कर्सर ऑब्जेक्ट (QtGui.QTextCursor का उदाहरण)\n- `html`: सम्मिलित करने के लिए HTML स्ट्रिंग\n\nमुख्य तर्क:\n1. कर्सर के लिए एक एडिट ब्लॉक शुरू करता है।\n2. HTML को कर्सर की वर्तमान स्थिति पर सम्मिलित करता है।\n3. HTML सम्मिलन के बाद, टेक्स्ट डॉक्युमेंट \"HTML मोड\" में रहता है, जिससे प्लेन टेक्स्ट जोड़ने पर अनचाहा फॉर्मेटिंग हो सकता है। इस समस्या को हल करने के लिए:\n   - कर्सर को बाईं ओर ले जाता है और चयन करता है।\n   - यदि चयनित टेक्स्ट एक स्पेस है, तो इसे हटा देता है।\n   - अन्यथा, कर्सर को दाईं ओर ले जाता है।\n   - एक खाली स्पेस को डिफ़ॉल्"
  },
  {
    "code": "def clinvar(institute_id, case_name, variant_id):\n    \"\"\"Build a clinVar submission form for a variant.\"\"\"\n    data = controllers.clinvar_export(store, institute_id, case_name, variant_id)\n    if request.method == 'GET':\n        return data\n    else: #POST\n        form_dict = request.form.to_dict()\n        submission_objects = set_submission_objects(form_dict) # A tuple of submission objects (variants and casedata objects)\n\n        # Add submission data to an open clinvar submission object,\n        # or create a new if no open submission is found in database\n        open_submission = store.get_open_clinvar_submission(current_user.email, institute_id)\n        updated_submission = store.add_to_submission(open_submission['_id'], submission_objects)\n\n        # Redirect to clinvar submissions handling page, and pass it the updated_submission_object\n        return redirect(url_for('cases.clinvar_submissions', institute_id=institute_id))",
    "docstring": "Build a clinVar submission form for a variant.",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "The function `clinvar` is designed to build and handle a ClinVar submission form for a specific variant. It takes three arguments: `institute_id` (string), `case_name` (string), and `variant_id` (string). The function first retrieves data using `controllers.clinvar_export` based on the provided arguments. If the request method is 'GET', it returns the retrieved data directly. If the request method is 'POST', it processes the form data, converts it to a dictionary, and uses it to create submission objects. It then checks for an open ClinVar submission associated with the current user and institute. If found, it adds the new submission data to the existing submission; otherwise, it creates a new submission. Finally, the function redirects to the ClinVar submissions handling page, passing the updated submission object.",
    "summary_chinese": "函数名为 clinvar，用于构建 ClinVar 变异提交表单。它接受三个参数：institute_id（机构 ID）、case_name（病例名称）和 variant_id（变异 ID）。函数的主要逻辑是：首先通过 controllers.clinvar_export 获取变异数据。如果请求方法是 GET，则直接返回数据；如果是 POST，则从请求表单中获取数据，并将其转换为提交对象。接着，检查数据库中是否存在未完成的 ClinVar 提交记录，如果存在则添加新数据，否则创建新记录。最后，重定向到 ClinVar 提交处理页面，并传递更新后的提交对象。",
    "summary_french": "La fonction `clinvar` a pour but de générer et de gérer un formulaire de soumission ClinVar pour une variante spécifique. Elle prend trois arguments : `institute_id` (identifiant de l'institut), `case_name` (nom du cas) et `variant_id` (identifiant de la variante). La fonction commence par récupérer les données nécessaires via `controllers.clinvar_export`. Si la requête est de type GET, elle retourne simplement ces données. En cas de requête POST, elle récupère les données du formulaire, les transforme en objets de soumission, puis les ajoute à une soumission ClinVar existante ou en crée une nouvelle si aucune n'est ouverte. Enfin, elle redirige l'utilisateur vers la page de gestion des soumissions ClinVar pour l'institut concerné.",
    "summary_spanish": "La función se llama \"clinvar\" y su propósito es construir y manejar un formulario de envío a ClinVar para una variante específica. Recibe tres argumentos: \"institute_id\" (identificador del instituto), \"case_name\" (nombre del caso) y \"variant_id\" (identificador de la variante), todos de tipo cadena o entero. La lógica principal de la función es la siguiente: primero, obtiene los datos necesarios para el formulario utilizando la función \"clinvar_export\". Si la solicitud es de tipo GET, devuelve los datos directamente. Si es de tipo POST, procesa el formulario enviado, crea objetos de envío y los agrega a un objeto de envío abierto en la base de datos, o crea uno nuevo si no existe. Finalmente, redirige al usuario a la página de manejo de envíos a ClinVar con el objeto de envío actualizado.",
    "summary_portuguese": "A função `clinvar` tem como objetivo construir e processar um formulário de submissão para o ClinVar, um banco de dados de variantes genéticas. Ela recebe três argumentos: `institute_id` (identificador da instituição), `case_name` (nome do caso) e `variant_id` (identificador da variante). A função inicialmente coleta dados usando `controllers.clinvar_export`. Se a requisição for do tipo GET, ela retorna os dados coletados. Caso seja uma requisição POST, ela processa o formulário enviado, cria objetos de submissão e os adiciona a uma submissão existente no banco de dados ou cria uma nova submissão se não houver uma aberta. Por fim, redireciona o usuário para a página de gerenciamento de submissões do ClinVar.",
    "summary_arabic": "اسم الدالة: clinvar\n\nوصف الدالة: تقوم هذه الدالة بإنشاء نموذج تقديم لـ ClinVar لمتغير معين.\n\nالحجج:\n- institute_id (سلسلة نصية أو رقم): معرف المعهد.\n- case_name (سلسلة نصية): اسم الحالة.\n- variant_id (سلسلة نصية أو رقم): معرف المتغير.\n\nالمنطق الرئيسي:\n1. يتم استدعاء دالة clinvar_export من وحدة controllers للحصول على بيانات المتغير.\n2. إذا كان طلب HTTP هو GET، يتم إرجاع البيانات مباشرة.\n3. إذا كان الطلب POST، يتم تحويل بيانات النموذج إلى قاموس.\n4. يتم استدعاء دالة set_submission_objects لإنشاء كائنات التقديم (متغيرات وبيانات الحالة).\n5. يتم البحث عن تقديم ClinVar مفتوح في قاعدة البيانات للمستخدم الحالي والمعهد.\n6. إذا تم العثور على تقديم مفتوح، يتم إضافة كائنات التقديم إليه. إذا لم يتم العثور على تقديم مفتوح، يتم إنشاء تقديم جديد.\n7. يتم إعادة توجيه المستخدم إلى صفحة معالجة تقديمات ClinVar مع تمرير كائن التقديم المحدث.",
    "summary_hindi": "clinvar फ़ंक्शन एक वेरिएंट के लिए ClinVar सबमिशन फॉर्म बनाने के लिए उपयोग किया जाता है। इसमें तीन तर्क होते हैं: institute_id (संस्थान की पहचान), case_name (मामले का नाम), और variant_id (वेरिएंट की पहचान)। यह फ़ंक्शन पहले controllers.clinvar_export का उपयोग करके डेटा प्राप्त करता है। यदि अनुरोध विधि GET है, तो यह डेटा वापस कर देता है। यदि अनुरोध विधि POST है, तो यह फॉर्म डेटा को एक शब्दकोश में परिवर्तित करता है और सबमिशन ऑब्जेक्ट्स सेट करता है। इसके बाद, यह जाँचता है कि क्या डेटाबेस में कोई खुला ClinVar सबमिशन है; यदि है, तो उसमें डेटा जोड़ता है, अन्यथा एक नया सबमिशन बनाता है। अंत में, यह उपयोगकर्ता को ClinVar सबमिशन हैंडलिंग पेज पर रीडायरेक्ट करता है।"
  },
  {
    "code": "def __write_to_fil_light(self, filename_out, *args, **kwargs):\n        \"\"\" Write data to .fil file.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        n_bytes  = self.header[b'nbits'] / 8\n        with open(filename_out, \"wb\") as fileh:\n            fileh.write(generate_sigproc_header(self)) #generate_sigproc_header comes from sigproc.py\n            j = self.data\n            if n_bytes == 4:\n                np.float32(j.ravel()).tofile(fileh)\n            elif n_bytes == 2:\n                np.int16(j.ravel()).tofile(fileh)\n            elif n_bytes == 1:\n                np.int8(j.ravel()).tofile(fileh)",
    "docstring": "Write data to .fil file.\n\n        Args:\n            filename_out (str): Name of output file",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "The function `__write_to_fil_light` writes data to a `.fil` file. It takes a filename as a string and additional arguments. The function first determines the number of bytes per data point from the header. It then opens the file in binary write mode and writes the generated SigProc header. Depending on the number of bytes (4, 2, or 1), the data is converted to the appropriate format (float32, int16, or int8) and written to the file. The key logic involves handling different data types based on the byte size specified in the header.",
    "summary_chinese": "函数名：__write_to_fil_light\n\n用途：将数据写入 .fil 文件。\n\n参数：\n- filename_out (str)：输出文件的名称。\n- *args：可变参数列表，未在函数中使用。\n- **kwargs：关键字参数字典，未在函数中使用。\n\n关键逻辑：\n该函数首先根据头部信息中的 'nbits' 字段计算出每个数据项的字节数。然后，它打开指定的输出文件，并写入由 generate_sigproc_header 函数生成的头部信息。接着，根据字节数的大小，将数据数组 j 转换为相应的 numpy 数据类型（np.float32、np.int16 或 np.int8），并将其写入文件。数据数组在写入前会被展平（ravel）。",
    "summary_french": "La fonction `__write_to_fil_light` permet d'écrire des données dans un fichier `.fil`. Elle prend en entrée le nom du fichier de sortie (`filename_out`, de type `str`) ainsi que des arguments supplémentaires (`*args` et `**kwargs`). La logique principale consiste à déterminer la taille des données en octets à partir de l'en-tête (`self.header[b'nbits'] / 8`), puis à écrire l'en-tête généré par `generate_sigproc_header` dans le fichier. Ensuite, les données (`self.data`) sont converties et écrites dans le fichier en fonction de la taille des octets : en `float32` si 4 octets, en `int16` si 2 octets, ou en `int8` si 1 octet.",
    "summary_spanish": "El nombre de la función es `__write_to_fil_light`. Su propósito es escribir datos en un archivo `.fil`. Los argumentos que recibe son `filename_out` (una cadena que indica el nombre del archivo de salida), `*args` (argumentos adicionales no especificados) y `**kwargs` (argumentos de palabra clave adicionales no especificados). La lógica principal de la función consiste en abrir el archivo especificado en modo de escritura binaria, escribir un encabezado generado por la función `generate_sigproc_header`, y luego escribir los datos contenidos en `self.data` en el archivo, convirtiéndolos al tipo de dato adecuado según el valor de `n_bytes` (que se calcula a partir del encabezado). Si `n_bytes` es 4, los datos se escriben como `float32`; si es 2, como `int16`; y si es 1, como `int8`.",
    "summary_portuguese": "O nome da função é `__write_to_fil_light`. Seu propósito é escrever dados em um arquivo `.fil`, incluindo um cabeçalho e os dados principais. A função recebe como argumentos `filename_out` (str), que é o nome do arquivo de saída, além de argumentos variáveis `*args` e `**kwargs`. A lógica principal envolve calcular o número de bytes com base no campo `nbits` do cabeçalho, abrir o arquivo em modo de escrita binária, escrever o cabeçalho gerado pela função `generate_sigproc_header`, e então escrever os dados (`self.data`) no arquivo, convertendo-os para o tipo adequado (float32, int16 ou int8) dependendo do número de bytes calculado.",
    "summary_arabic": "اسم الدالة: __write_to_fil_light\n\nوصف الدالة: تقوم هذه الدالة بكتابة البيانات إلى ملف بامتداد .fil.\n\nالحجج:\n\n* filename_out (str): اسم ملف الإخراج.\n* *args: حجج إضافية (غير مستخدمة في هذه الدالة).\n* **kwargs: كلمات رئيسية إضافية (غير مستخدمة في هذه الدالة).\n\nالمنطق الرئيسي:\n\n1. تحسب الدالة عدد البايتات بناءً على قيمة 'nbits' من الرأس (header).\n2. تفتح الدالة ملف الإخراج للكتابة في الوضع الثنائي.\n3. تكتب الدالة رأس الملف باستخدام الدالة generate_sigproc_header.\n4. تحدد الدالة نوع البيانات بناءً على عدد البايتات:\n   * إذا كان عدد البايتات 4، تكتب البيانات كقيم float32.\n   * إذا كان عدد البايتات 2، تكتب البيانات كقيم int16.\n   * إذا كان عدد البايتات 1، تكتب البيانات كقيم int8.\n5. تستخدم الدالة الدالة ravel() لتحويل البيانات إلى مصفوفة أحادية البعد قبل الكتابة إلى الملف.",
    "summary_hindi": "फ़ंक्शन का नाम: `__write_to_fil_light`  \nउद्देश्य: यह फ़ंक्शन डेटा को `.fil` फ़ाइल में लिखता है।  \nआर्ग्युमेंट्स:  \n- `filename_out` (str): आउटपुट फ़ाइल का नाम।  \n- `*args` और `**kwargs`: अतिरिक्त आर्ग्युमेंट्स (इस फ़ंक्शन में उपयोग नहीं किए गए हैं)।  \n\nमुख्य तर्क:  \n1. `n_bytes` की गणना `header` में `nbits` को 8 से भाग देकर की जाती है।  \n2. `filename_out` के साथ एक फ़ाइल खोली जाती है और `generate_sigproc_header` फ़ंक्शन का उपयोग करके हेडर लिखा जाता है।  \n3. `self.data` को `j` में स्टोर किया जाता है।  \n4. `n_bytes` के मान के आधार पर डेटा को अलग-अलग डेटा टाइप (`np.float32`, `np.int16`, या `np.int8`) में कन्वर्ट करके फ़ाइल में लिखा जाता है।"
  },
  {
    "code": "def build_factored_variational_loss(model,\n                                    observed_time_series,\n                                    init_batch_shape=(),\n                                    seed=None,\n                                    name=None):\n  \"\"\"Build a loss function for variational inference in STS models.\n\n  Variational inference searches for the distribution within some family of\n  approximate posteriors that minimizes a divergence between the approximate\n  posterior `q(z)` and true posterior `p(z|observed_time_series)`. By converting\n  inference to optimization, it's generally much faster than sampling-based\n  inference algorithms such as HMC. The tradeoff is that the approximating\n  family rarely contains the true posterior, so it may miss important aspects of\n  posterior structure (in particular, dependence between variables) and should\n  not be blindly trusted. Results may vary; it's generally wise to compare to\n  HMC to evaluate whether inference quality is sufficient for your task at hand.\n\n  This method constructs a loss function for variational inference using the\n  Kullback-Liebler divergence `KL[q(z) || p(z|observed_time_series)]`, with an\n  approximating family given by independent Normal distributions transformed to\n  the appropriate parameter space for each parameter. Minimizing this loss (the\n  negative ELBO) maximizes a lower bound on the log model evidence `-log\n  p(observed_time_series)`. This is equivalent to the 'mean-field' method\n  implemented in [1]. and is a standard approach. The resulting posterior\n  approximations are unimodal; they will tend to underestimate posterior\n  uncertainty when the true posterior contains multiple modes (the `KL[q||p]`\n  divergence encourages choosing a single mode) or dependence between variables.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries`, which includes\n      a mask `Tensor` to specify timesteps with missing observations.\n    init_batch_shape: Batch shape (Python `tuple`, `list`, or `int`) of initial\n      states to optimize in parallel.\n      Default value: `()`. (i.e., just run a single optimization).\n    seed: Python integer to seed the random number generator.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., 'build_factored_variational_loss').\n\n  Returns:\n    variational_loss: `float` `Tensor` of shape\n      `concat([init_batch_shape, model.batch_shape])`, encoding a stochastic\n      estimate of an upper bound on the negative model evidence `-log p(y)`.\n      Minimizing this loss performs variational inference; the gap between the\n      variational bound and the true (generally unknown) model evidence\n      corresponds to the divergence `KL[q||p]` between the approximate and true\n      posterior.\n    variational_distributions: `collections.OrderedDict` giving\n      the approximate posterior for each model parameter. The keys are\n      Python `str` parameter names in order, corresponding to\n      `[param.name for param in model.parameters]`. The values are\n      `tfd.Distribution` instances with batch shape\n      `concat([init_batch_shape, model.batch_shape])`; these will typically be\n      of the form `tfd.TransformedDistribution(tfd.Normal(...),\n      bijector=param.bijector)`.\n\n  #### Examples\n\n  Assume we've built a structural time-series model:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n  ```\n\n  To run variational inference, we simply construct the loss and optimize\n  it:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} loss {}\".format(step, loss_))\n\n      posterior_samples_ = sess.run({\n        param_name: q.sample(50)\n        for param_name, q in variational_distributions.items()})\n  ```\n\n  As a more complex example, we might try to avoid local optima by optimizing\n  from multiple initializations in parallel, and selecting the result with the\n  lowest loss:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series,\n       init_batch_shape=[10])\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} losses {}\".format(step, loss_))\n\n      # Draw multiple samples to reduce Monte Carlo error in the optimized\n      # variational bounds.\n      avg_loss = np.mean(\n        [sess.run(variational_loss) for _ in range(25)], axis=0)\n      best_posterior_idx = np.argmin(avg_loss, axis=0).astype(np.int32)\n  ```\n\n  #### References\n\n  [1]: Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and\n       David M. Blei. Automatic Differentiation Variational Inference. In\n       _Journal of Machine Learning Research_, 2017.\n       https://arxiv.org/abs/1603.00788\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(\n      name, 'build_factored_variational_loss',\n      values=[observed_time_series]) as name:\n    seed = tfd.SeedStream(\n        seed, salt='StructuralTimeSeries_build_factored_variational_loss')\n\n    variational_distributions = collections.OrderedDict()\n    variational_samples = []\n    for param in model.parameters:\n      def initial_loc_fn(param):\n        return sample_uniform_initial_state(\n            param, return_constrained=True,\n            init_sample_shape=init_batch_shape,\n            seed=seed())\n      q = _build_trainable_posterior(param, initial_loc_fn=initial_loc_fn)\n      variational_distributions[param.name] = q\n      variational_samples.append(q.sample(seed=seed()))\n\n    # Multiple initializations (similar to HMC chains) manifest as an extra\n    # param batch dimension, so we need to add corresponding batch dimension(s)\n    # to `observed_time_series`.\n    observed_time_series = sts_util.pad_batch_dimension_for_multiple_chains(\n        observed_time_series, model, chain_batch_shape=init_batch_shape)\n\n    # Construct the variational bound.\n    log_prob_fn = model.joint_log_prob(observed_time_series)\n    expected_log_joint = log_prob_fn(*variational_samples)\n    entropy = tf.reduce_sum(\n        input_tensor=[\n            -q.log_prob(sample) for (q, sample) in zip(\n                variational_distributions.values(), variational_samples)\n        ],\n        axis=0)\n    variational_loss = -(expected_log_joint + entropy)  # -ELBO\n\n  return variational_loss, variational_distributions",
    "docstring": "Build a loss function for variational inference in STS models.\n\n  Variational inference searches for the distribution within some family of\n  approximate posteriors that minimizes a divergence between the approximate\n  posterior `q(z)` and true posterior `p(z|observed_time_series)`. By converting\n  inference to optimization, it's generally much faster than sampling-based\n  inference algorithms such as HMC. The tradeoff is that the approximating\n  family rarely contains the true posterior, so it may miss important aspects of\n  posterior structure (in particular, dependence between variables) and should\n  not be blindly trusted. Results may vary; it's generally wise to compare to\n  HMC to evaluate whether inference quality is sufficient for your task at hand.\n\n  This method constructs a loss function for variational inference using the\n  Kullback-Liebler divergence `KL[q(z) || p(z|observed_time_series)]`, with an\n  approximating family given by independent Normal distributions transformed to\n  the appropriate parameter space for each parameter. Minimizing this loss (the\n  negative ELBO) maximizes a lower bound on the log model evidence `-log\n  p(observed_time_series)`. This is equivalent to the 'mean-field' method\n  implemented in [1]. and is a standard approach. The resulting posterior\n  approximations are unimodal; they will tend to underestimate posterior\n  uncertainty when the true posterior contains multiple modes (the `KL[q||p]`\n  divergence encourages choosing a single mode) or dependence between variables.\n\n  Args:\n    model: An instance of `StructuralTimeSeries` representing a\n      time-series model. This represents a joint distribution over\n      time-series and their parameters with batch shape `[b1, ..., bN]`.\n    observed_time_series: `float` `Tensor` of shape\n      `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where\n      `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`\n      dimension may (optionally) be omitted if `num_timesteps > 1`. May\n      optionally be an instance of `tfp.sts.MaskedTimeSeries`, which includes\n      a mask `Tensor` to specify timesteps with missing observations.\n    init_batch_shape: Batch shape (Python `tuple`, `list`, or `int`) of initial\n      states to optimize in parallel.\n      Default value: `()`. (i.e., just run a single optimization).\n    seed: Python integer to seed the random number generator.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., 'build_factored_variational_loss').\n\n  Returns:\n    variational_loss: `float` `Tensor` of shape\n      `concat([init_batch_shape, model.batch_shape])`, encoding a stochastic\n      estimate of an upper bound on the negative model evidence `-log p(y)`.\n      Minimizing this loss performs variational inference; the gap between the\n      variational bound and the true (generally unknown) model evidence\n      corresponds to the divergence `KL[q||p]` between the approximate and true\n      posterior.\n    variational_distributions: `collections.OrderedDict` giving\n      the approximate posterior for each model parameter. The keys are\n      Python `str` parameter names in order, corresponding to\n      `[param.name for param in model.parameters]`. The values are\n      `tfd.Distribution` instances with batch shape\n      `concat([init_batch_shape, model.batch_shape])`; these will typically be\n      of the form `tfd.TransformedDistribution(tfd.Normal(...),\n      bijector=param.bijector)`.\n\n  #### Examples\n\n  Assume we've built a structural time-series model:\n\n  ```python\n    day_of_week = tfp.sts.Seasonal(\n        num_seasons=7,\n        observed_time_series=observed_time_series,\n        name='day_of_week')\n    local_linear_trend = tfp.sts.LocalLinearTrend(\n        observed_time_series=observed_time_series,\n        name='local_linear_trend')\n    model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],\n                        observed_time_series=observed_time_series)\n  ```\n\n  To run variational inference, we simply construct the loss and optimize\n  it:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series)\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} loss {}\".format(step, loss_))\n\n      posterior_samples_ = sess.run({\n        param_name: q.sample(50)\n        for param_name, q in variational_distributions.items()})\n  ```\n\n  As a more complex example, we might try to avoid local optima by optimizing\n  from multiple initializations in parallel, and selecting the result with the\n  lowest loss:\n\n  ```python\n    (variational_loss,\n     variational_distributions) = tfp.sts.build_factored_variational_loss(\n       model=model, observed_time_series=observed_time_series,\n       init_batch_shape=[10])\n\n    train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for step in range(200):\n        _, loss_ = sess.run((train_op, variational_loss))\n\n        if step % 20 == 0:\n          print(\"step {} losses {}\".format(step, loss_))\n\n      # Draw multiple samples to reduce Monte Carlo error in the optimized\n      # variational bounds.\n      avg_loss = np.mean(\n        [sess.run(variational_loss) for _ in range(25)], axis=0)\n      best_posterior_idx = np.argmin(avg_loss, axis=0).astype(np.int32)\n  ```\n\n  #### References\n\n  [1]: Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and\n       David M. Blei. Automatic Differentiation Variational Inference. In\n       _Journal of Machine Learning Research_, 2017.\n       https://arxiv.org/abs/1603.00788",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "Function name: build_factored_variational_loss\n\nPurpose: Constructs a loss function for variational inference in Structural Time Series (STS) models, enabling optimization-based approximation of the posterior distribution.\n\nArguments:\n- model (StructuralTimeSeries): Represents the time-series model, defining the joint distribution over time-series and parameters.\n- observed_time_series (float Tensor or MaskedTimeSeries): Contains the observed time-series data, optionally including a mask for missing values.\n- init_batch_shape (tuple, list, or int, optional): Specifies the batch shape for initial states to optimize in parallel, defaulting to an empty tuple.\n- seed (int, optional): Seeds the random number generator for reproducibility.\n- name (str, optional): Prefixes ops created by this function, defaulting to 'build_factored_variational_loss'.\n\nKey logic:\n1. Initializes variational distributions for each model parameter using independent Normal distributions transformed to the appropriate parameter space.\n2. Constructs the variational loss (negative ELBO) by computing the expected log joint probability and entropy of the variational distributions.\n3. Adjusts the observed_time_series to match the batch shape of multiple initializations if necessary.\n4. Returns the variational loss and an OrderedDict of variational distributions for each parameter, enabling optimization and posterior approximation.",
    "summary_chinese": "函数名称：build_factored_variational_loss\n\n函数用途：构建用于结构化时间序列模型（STS）中变分推断的损失函数。通过最小化该损失函数，可以近似后验分布，从而加速推断过程。\n\n参数列表：\n- model：结构化时间序列模型的实例，表示时间序列及其参数的联合分布。\n- observed_time_series：观测时间序列数据，可以是浮点数张量或MaskedTimeSeries实例。\n- init_batch_shape：初始状态的批次形状，默认为空元组。\n- seed：随机数生成器的种子，默认为None。\n- name：操作的名称前缀，默认为None。\n\n关键逻辑：\n1. 为每个模型参数构建可训练的后验分布，并采样得到变分样本。\n2. 调整观测时间序列的批次维度，以适应多个初始化。\n3. 构建变分下界（ELBO），包括计算联合对数概率和熵。\n4. 返回变分损失（负ELBO）和变分分布字典。\n\n该函数通过最小化变分损失，实现变分推断，从而近似后验分布。",
    "summary_french": "Nom de la fonction : build_factored_variational_loss\n\nDescription : Cette fonction construit une fonction de perte pour l'inférence variationnelle dans les modèles de séries temporelles structurelles (STS). Elle utilise la divergence de Kullback-Leibler pour approximer la distribution postérieure des paramètres du modèle.\n\nArguments :\n- model (StructuralTimeSeries) : un modèle de série temporelle structurelle.\n- observed_time_series (float Tensor ou MaskedTimeSeries) : les données observées de la série temporelle.\n- init_batch_shape (tuple, list ou int, optionnel) : la forme du batch initial pour l'optimisation parallèle. Par défaut, ().\n- seed (int, optionnel) : une graine pour le générateur de nombres aléatoires.\n- name (str, optionnel) : un nom pour les opérations créées par cette fonction. Par défaut, 'build_factored_variational_loss'.\n\nLogique clé :\n1. Initialisation des distributions variationnelles pour chaque paramètre du modèle en utilisant des distributions normales transformées.\n2. Échantillonnage des valeurs des paramètres à partir des distributions variationnelles.\n3. Construction de la fonction de perte variationnelle en utilisant la divergence de Kullback-Leibler et l'entropie des distributions variationnelles.\n4. Retour de la fonction de perte et des distributions variationnelles pour chaque paramètre du modèle.\n\nLa fonction build_factored_variational_loss permet de réaliser une inférence variationnelle dans les modèles de séries temporelles structurelles en minimisant la fonction de perte retournée. Les distributions variationnelles approximatives sont utilisées pour estimer les paramètres du modèle, et la fonction de perte mesure la divergence entre ces distributions et la distribution postérieure vraie.",
    "summary_spanish": "Nombre de la función: build_factored_variational_loss\n\nPropósito: Construir una función de pérdida para inferencia variacional en modelos de series temporales estructurales (STS). La inferencia variacional busca la distribución dentro de una familia de aproximaciones posteriores que minimiza la divergencia entre la aproximación posterior y la verdadera posterior.\n\nArgumentos:\n- model (StructuralTimeSeries): Instancia de un modelo de series temporales estructurales.\n- observed_time_series (float Tensor o MaskedTimeSeries): Serie temporal observada.\n- init_batch_shape (tuple, list o int, opcional): Forma del lote inicial para optimización paralela. Por defecto, ().\n- seed (int, opcional): Semilla para el generador de números aleatorios.\n- name (str, opcional): Nombre para las operaciones creadas por la función. Por defecto, None.\n\nLógica clave:\n1. Se inicializan las distribuciones variacionales para cada parámetro del modelo utilizando distribuciones normales transformadas.\n2. Se construyen las muestras variacionales a partir de las distribuciones variacionales.\n3. Se ajusta la forma del lote de la serie temporal observada para múltiples inicializaciones.\n4. Se calcula la función de pérdida variacional utilizando la divergencia de Kullback-Leibler y la entropía de las distribuciones variacionales.\n5. Se devuelve la función de pérdida variacional y las distribuciones variacionales.\n\nLa función construye una pérdida que, al minimizarse, realiza inferencia variacional, aproximando la verdadera posterior del modelo.",
    "summary_portuguese": "Nome da função: build_factored_variational_loss\n\nDescrição: Constrói uma função de perda para inferência variacional em modelos de séries temporais estruturais (STS). A inferência variacional busca a distribuição dentro de uma família de pós-distribuições aproximadas que minimiza a divergência entre a pós-distribuição aproximada e a verdadeira pós-distribuição.\n\nArgumentos:\n- model (StructuralTimeSeries): Instância de um modelo de série temporal estrutural.\n- observed_time_series (float Tensor ou MaskedTimeSeries): Série temporal observada.\n- init_batch_shape (tuple, list ou int, opcional): Forma do lote inicial para otimização paralela. Padrão é ().\n- seed (int, opcional): Semente para o gerador de números aleatórios.\n- name (str, opcional): Nome para as operações criadas pela função. Padrão é None.\n\nLógica principal:\n1. Inicializa as distribuições variacionais para cada parâmetro do modelo usando distribuições normais transformadas.\n2. Ajusta a série temporal observada para lidar com múltiplas inicializações.\n3. Constrói a função de perda variacional (ELBO) calculando a expectativa do log da probabilidade conjunta e a entropia das distribuições variacionais.\n4. Retorna a perda variacional e as distribuições variacionais aproximadas para cada parâmetro do modelo.\n\nA função é usada para realizar inferência variacional em modelos STS, fornecendo uma estimativa da perda e as distribuições pós-variacionais aproximadas.",
    "summary_arabic": "اسم الدالة: build_factored_variational_loss\n\nالغرض: بناء دالة خسارة للاستدلال التبايني في نماذج السلاسل الزمنية الهيكلية (STS). يتم استخدام هذه الدالة لتحسين توزيعات خلفية تقريبية لتقليل التباعد بين التوزيع الخلفي التقريبي والتوزيع الخلفي الحقيقي.\n\nالحجج:\n- model: مثيل من الفئة StructuralTimeSeries يمثل نموذج سلسلة زمنية.\n- observed_time_series: مصفوفة من النوع float تمثل الملاحظات الزمنية.\n- init_batch_shape: شكل الدفعة الأولية للحالات التي سيتم تحسينها بشكل متوازٍ (القيمة الافتراضية: ()).\n- seed: بذرة مولد الأعداد العشوائية (القيمة الافتراضية: None).\n- name: اسم الدالة (القيمة الافتراضية: None).\n\nالمنطق الرئيسي:\n1. يتم إنشاء توزيعات خلفية تقريبية لكل معلمة في النموذج باستخدام دالة _build_trainable_posterior.\n2. يتم أخذ عينات من هذه التوزيعات التقريبية.\n3. يتم تعديل الملاحظات الزمنية لإضافة أبعاد دفعة إضافية إذا لزم الأمر.\n4. يتم حساب الدالة log_prob_fn باستخدام الدالة joint_log_prob للنموذج.\n5. يتم حساب القيمة المتوقعة للوجست المشترك (expected_log_joint) باستخدام العينات التقريبية.\n6. يتم حساب الإنتروبيا (entropy) باستخدام التوزيعات التقريبية والعينات.\n7. يتم حساب دالة الخسارة التباينية (variational_loss) كنفي للقيمة المتوقعة للوجست المشترك زائد الإنتروبيا ( -ELBO).\n\nتعيد الدالة دالة الخسارة التباينية والتوزيعات الخلفية التقريبية.",
    "summary_hindi": "फ़ंक्शन का नाम: `build_factored_variational_loss`\n\nउद्देश्य: यह फ़ंक्शन संरचनात्मक समय-श्रृंखला मॉडल (STS) में वेरिएशनल इन्फ़रेंस के लिए एक लॉस फ़ंक्शन बनाता है। यह लॉस फ़ंक्शन, Kullback-Leibler डाइवर्जेंस का उपयोग करके, अप्रोक्सिमेट पोस्टीरियर और सच्चे पोस्टीरियर के बीच के अंतर को कम करने का प्रयास करता है।\n\nआर्ग्युमेंट्स:\n- `model`: `StructuralTimeSeries` का एक उदाहरण, जो एक समय-श्रृंखला मॉडल का प्रतिनिधित्व करता है।\n- `observed_time_series`: `float` टाइप का `Tensor`, जो अवलोकन के समय-श्रृंखला डेटा को दर्शाता है।\n- `init_batch_shape`: प्रारंभिक स्थितियों का बैच आकार, डिफ़ॉल्ट मान `()` है।\n- `seed`: यादृच्छिक संख्या जनरेटर के लिए बीज मान, डिफ़ॉल्ट मान `None` है।\n- `name`: फ़ंक्शन द्वारा बनाए गए ऑप्स के लिए नाम, डिफ़ॉल्ट मान `None` है।\n\nमुख्य तर्क:\n1. **वेरिएशनल वितरण का निर्माण**: प्रत्येक मॉडल पैर"
  },
  {
    "code": "def find_expcoef(self, nsd_below=0., plot=False,\n                     trimlim=None, autorange_kwargs={}):\n        \"\"\"\n        Determines exponential decay coefficient for despike filter.\n\n        Fits an exponential decay function to the washout phase of standards\n        to determine the washout time of your laser cell. The exponential\n        coefficient reported is `nsd_below` standard deviations below the\n        fitted exponent, to ensure that no real data is removed.\n\n        Total counts are used in fitting, rather than a specific analyte.\n\n        Parameters\n        ----------\n        nsd_below : float\n            The number of standard deviations to subtract from the fitted\n            coefficient when calculating the filter exponent.\n        plot : bool or str\n            If True, creates a plot of the fit, if str the plot is to the\n            location specified in str.\n        trimlim : float\n            A threshold limit used in determining the start of the\n            exponential decay region of the washout. Defaults to half\n            the increase in signal over background. If the data in\n            the plot don't fall on an exponential decay line, change\n            this number. Normally you'll need to increase it.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        print('Calculating exponential decay coefficient\\nfrom SRM washouts...')\n\n        def findtrim(tr, lim=None):\n            trr = np.roll(tr, -1)\n            trr[-1] = 0\n            if lim is None:\n                lim = 0.5 * np.nanmax(tr - trr)\n            ind = (tr - trr) >= lim\n            return np.arange(len(ind))[ind ^ np.roll(ind, -1)][0]\n\n        if not hasattr(self.stds[0], 'trnrng'):\n            for s in self.stds:\n                s.autorange(**autorange_kwargs, ploterrs=False)\n\n        trans = []\n        times = []\n        for v in self.stds:\n            for trnrng in v.trnrng[-1::-2]:\n                tr = minmax_scale(v.data['total_counts'][(v.Time > trnrng[0]) & (v.Time < trnrng[1])])\n                sm = np.apply_along_axis(np.nanmean, 1,\n                                         rolling_window(tr, 3, pad=0))\n                sm[0] = sm[1]\n                trim = findtrim(sm, trimlim) + 2\n                trans.append(minmax_scale(tr[trim:]))\n                times.append(np.arange(tr[trim:].size) *\n                             np.diff(v.Time[1:3]))\n\n        times = np.concatenate(times)\n        times = np.round(times, 2)\n        trans = np.concatenate(trans)\n\n        ti = []\n        tr = []\n        for t in np.unique(times):\n            ti.append(t)\n            tr.append(np.nanmin(trans[times == t]))\n\n        def expfit(x, e):\n            \"\"\"\n            Exponential decay function.\n            \"\"\"\n            return np.exp(e * x)\n\n        ep, ecov = curve_fit(expfit, ti, tr, p0=(-1.))\n\n        eeR2 = R2calc(trans, expfit(times, ep))\n\n        if plot:\n            fig, ax = plt.subplots(1, 1, figsize=[6, 4])\n\n            ax.scatter(times, trans, alpha=0.2, color='k', marker='x', zorder=-2)\n            ax.scatter(ti, tr, alpha=1, color='k', marker='o')\n            fitx = np.linspace(0, max(ti))\n            ax.plot(fitx, expfit(fitx, ep), color='r', label='Fit')\n            ax.plot(fitx, expfit(fitx, ep - nsd_below * np.diag(ecov)**.5, ),\n                    color='b', label='Used')\n            ax.text(0.95, 0.75,\n                    ('y = $e^{%.2f \\pm %.2f * x}$\\n$R^2$= %.2f \\nCoefficient: '\n                     '%.2f') % (ep,\n                                np.diag(ecov)**.5,\n                                eeR2,\n                                ep - nsd_below * np.diag(ecov)**.5),\n                    transform=ax.transAxes, ha='right', va='top', size=12)\n            ax.set_xlim(0, ax.get_xlim()[-1])\n            ax.set_xlabel('Time (s)')\n            ax.set_ylim(-0.05, 1.05)\n            ax.set_ylabel('Proportion of Signal')\n            plt.legend()\n            if isinstance(plot, str):\n                fig.savefig(plot)\n\n        self.expdecay_coef = ep - nsd_below * np.diag(ecov)**.5\n\n        print('  {:0.2f}'.format(self.expdecay_coef[0]))\n\n        return",
    "docstring": "Determines exponential decay coefficient for despike filter.\n\n        Fits an exponential decay function to the washout phase of standards\n        to determine the washout time of your laser cell. The exponential\n        coefficient reported is `nsd_below` standard deviations below the\n        fitted exponent, to ensure that no real data is removed.\n\n        Total counts are used in fitting, rather than a specific analyte.\n\n        Parameters\n        ----------\n        nsd_below : float\n            The number of standard deviations to subtract from the fitted\n            coefficient when calculating the filter exponent.\n        plot : bool or str\n            If True, creates a plot of the fit, if str the plot is to the\n            location specified in str.\n        trimlim : float\n            A threshold limit used in determining the start of the\n            exponential decay region of the washout. Defaults to half\n            the increase in signal over background. If the data in\n            the plot don't fall on an exponential decay line, change\n            this number. Normally you'll need to increase it.\n\n        Returns\n        -------\n        None",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "Function name: find_expcoef\n\nPurpose: Determines the exponential decay coefficient for a despike filter by fitting an exponential decay function to the washout phase of standards.\n\nArguments:\n- nsd_below (float): Number of standard deviations to subtract from the fitted coefficient.\n- plot (bool or str): If True, creates a plot of the fit; if str, saves the plot to the specified location.\n- trimlim (float): Threshold limit used to determine the start of the exponential decay region.\n- autorange_kwargs (dict): Keyword arguments for the autorange method.\n\nKey logic:\n1. Preprocesses standard data by applying autorange if necessary and scaling total counts.\n2. Identifies the start of the exponential decay region using a rolling window and a threshold limit.\n3. Concatenates and scales time and transmission data from all standards.\n4. Fits an exponential decay function to the data using curve_fit and calculates the R-squared value.\n5. Adjusts the fitted coefficient by subtracting the specified number of standard deviations.\n6. Optionally plots the fit, including the adjusted coefficient, and saves the plot if requested.\n7. Stores the adjusted exponential decay coefficient in the instance variable expdecay_coef.",
    "summary_chinese": "函数名称：find_expcoef\n\n函数用途：用于确定去尖滤波器的指数衰减系数。通过拟合标准品洗脱阶段的指数衰减函数，来确定激光细胞的洗脱时间。报告的指数系数是拟合指数减去 `nsd_below` 个标准差，以确保不移除真实数据。\n\n参数：\n- nsd_below（浮点数）：从拟合系数中减去的标准差数量，用于计算滤波指数。\n- plot（布尔值或字符串）：如果为 True，则生成拟合图；如果为字符串，则将图保存到指定位置。\n- trimlim（浮点数）：用于确定洗脱指数衰减区域起始点的阈值限制。默认为信号相对于背景增加的一半。\n- autorange_kwargs（字典）：自动范围调整的关键字参数。\n\n关键逻辑：\n1. 定义一个辅助函数 `findtrim`，用于找到指数衰减区域的起始点。\n2. 如果标准品对象没有 `trnrng` 属性，则为其自动设置范围。\n3. 提取标准品数据中的传输信号和时间信息，并进行归一化处理。\n4. 使用 `curve_fit` 函数拟合指数衰减函数，并计算拟合优度。\n5. 如果指定了 `plot` 参数，则生成拟合图，并显示拟合曲线、使用曲线以及相关统计信息。\n6. 计算并返回指数衰减系数，同时打印结果。",
    "summary_french": "Nom de la fonction : find_expcoef\n\nDescription : Détermine le coefficient de décroissance exponentielle pour un filtre de suppression des pics.\n\nArguments :\n- nsd_below (float) : nombre d'écarts-types à soustraire du coefficient ajusté pour calculer l'exposant du filtre.\n- plot (bool ou str) : si True, crée un graphique de l'ajustement ; si str, enregistre le graphique à l'emplacement spécifié.\n- trimlim (float) : seuil utilisé pour déterminer le début de la région de décroissance exponentielle. Par défaut, la moitié de l'augmentation du signal par rapport au bruit de fond.\n- autorange_kwargs (dict) : arguments supplémentaires pour la fonction autorange.\n\nLogique principale :\nLa fonction analyse les données de lavage des standards pour ajuster une fonction de décroissance exponentielle. Elle utilise les comptages totaux plutôt qu'un analyte spécifique. La fonction trouve le début de la région de décroissance exponentielle en utilisant un seuil (trimlim), puis ajuste une fonction exponentielle aux données. Le coefficient de décroissance exponentielle est calculé en soustrayant un certain nombre d'écarts-types (nsd_below) du coefficient ajusté. Si plot est True, un graphique de l'ajustement est créé. La fonction retourne le coefficient de décroissance exponentielle et l'affiche également.",
    "summary_spanish": "Nombre de la función: find_expcoef\n\nDescripción: Determina el coeficiente de decaimiento exponencial para un filtro de eliminación de picos en datos de lavado de estándares. Este coeficiente se utiliza para ajustar una función de decaimiento exponencial a la fase de lavado, asegurando que no se elimine datos reales al aplicar el filtro.\n\nArgumentos:\n- nsd_below (float): Número de desviaciones estándar a restar del coeficiente ajustado para calcular el exponente del filtro.\n- plot (bool o str): Si es True, genera un gráfico del ajuste; si es str, guarda el gráfico en la ubicación especificada.\n- trimlim (float): Umbral para determinar el inicio de la región de decaimiento exponencial. Por defecto, es la mitad del aumento de señal sobre el fondo.\n- autorange_kwargs (dict): Argumentos adicionales para el método autorange.\n\nLógica clave:\n1. Normaliza y suaviza los datos de conteo total de los estándares.\n2. Determina el inicio de la región de decaimiento exponencial usando el umbral trimlim.\n3. Ajusta una función de decaimiento exponencial a los datos de la región de lavado.\n4. Calcula el coeficiente de decaimiento exponencial restando nsd_below veces la desviación estándar del coeficiente ajustado.\n5. Opcionalmente, genera un gráfico del ajuste y muestra el coeficiente calculado.\n6. Almacena el coeficiente en self.expdecay_coef y lo imprime.",
    "summary_portuguese": "Nome da função: find_expcoef\n\nDescrição: Determina o coeficiente de decaimento exponencial para um filtro de remoção de picos (despike filter) a partir de dados de lavagem (washout) de padrões.\n\nArgumentos:\n- nsd_below (float): Número de desvios padrão a subtrair do coeficiente ajustado.\n- plot (bool ou str): Se True, gera um gráfico do ajuste; se str, salva o gráfico no local especificado.\n- trimlim (float): Limite para determinar o início da região de decaimento exponencial.\n- autorange_kwargs (dict): Argumentos para a função autorange.\n\nLógica principal:\n1. Normaliza e suaviza os dados de contagem total de cada padrão.\n2. Identifica o início da região de decaimento exponencial usando a função findtrim.\n3. Ajusta uma função de decaimento exponencial aos dados de decaimento.\n4. Calcula o coeficiente de decaimento exponencial, subtraindo nsd_below desvios padrão do coeficiente ajustado.\n5. Opcionalmente, gera um gráfico do ajuste e salva o coeficiente calculado no atributo expdecay_coef do objeto.",
    "summary_arabic": "اسم الدالة: find_expcoef\n\nوصف الدالة: تحدد معامل الانحلال الأسي لمرشح إزالة التشويش. تقوم بملاءمة دالة الانحلال الأسي لمرحلة الغسل في المعايير لتحديد وقت غسل خلية الليزر. يتم الإبلاغ عن المعامل الأسي الذي يقل عن الانحراف المعياري المحدد بـ `nsd_below` لضمان عدم إزالة أي بيانات حقيقية.\n\nالحجج:\n- nsd_below (float): عدد الانحرافات المعيارية التي يتم طرحها من المعامل الملائم عند حساب أس الانحلال.\n- plot (bool or str): إذا كان True، يقوم بإنشاء رسم بياني للملاءمة، وإذا كان str، يتم حفظ الرسم البياني في الموقع المحدد في السلسلة.\n- trimlim (float): عتبة تستخدم لتحديد بداية منطقة الانحلال الأسي في الغسل. القيمة الافتراضية هي نصف الزيادة في الإشارة فوق الخلفية.\n- autorange_kwargs (dict): معلمات إضافية لدالة autorange.\n\nالمنطق الرئيسي:\n1. تقوم الدالة بحساب معامل الانحلال الأسي من غسل المعايير.\n2. يتم تحديد بداية منطقة الانحلال الأسي باستخدام دالة findtrim.\n3. يتم تجميع البيانات من جميع المعايير وإعدادها للملاءمة.\n4. يتم ملاءمة دالة الانحلال الأسي للبيانات باستخدام curve_fit.\n5. يتم حساب معامل التحديد (R^2) لتقييم جودة الملاءمة.\n6. إذا تم تحديد plot، يتم إنشاء رسم بياني للملاءمة مع البيانات الأصلية والملاءمة.\n7. يتم حفظ معامل الانحلال الأسي في خاصية الكائن self.expdecay_coef.\n8. يتم طباعة قيمة المعامل الأسي.",
    "summary_hindi": "फ़ंक्शन का नाम: `find_expcoef`\n\nउद्देश्य: यह फ़ंक्शन डिस्पाइक फ़िल्टर के लिए एक्सपोनेंशियल डिके कोएफ़िशिएंट को निर्धारित करता है। यह स्टैंडर्ड्स के वॉशआउट चरण पर एक्सपोनेंशियल डिके फ़ंक्शन को फ़िट करके लेजर सेल के वॉशआउट समय को निर्धारित करता है।\n\nआर्ग्युमेंट्स:\n- `nsd_below` (float): फ़िटेड कोएफ़िशिएंट से घटाए जाने वाले स्टैंडर्ड डिविएशन की संख्या।\n- `plot` (bool या str): अगर True, तो फ़िट का प्लॉट बनाता है; अगर str, तो प्लॉट को str में निर्दिष्ट स्थान पर सहेजता है।\n- `trimlim` (float): वॉशआउट के एक्सपोनेंशियल डिके क्षेत्र की शुरुआत को निर्धारित करने के लिए एक थ्रेशोल्ड सीमा।\n- `autorange_kwargs` (dict): `autorange` फ़ंक्शन के लिए अतिरिक्त कीवर्ड आर्ग्युमेंट्स।\n\nमुख्य तर्क:\n1. फ़ंक्शन `findtrim` का उपयोग करके एक्सपोनेंशियल डिके क्षेत्र की शुरुआत को निर्धारित"
  },
  {
    "code": "def statements_to_HWProcesses(statements: List[HdlStatement])\\\n        -> Generator[HWProcess, None, None]:\n    \"\"\"\n    Pack statements into HWProcess instances,\n    * for each out signal resolve it's drivers and collect them\n    * split statements if there is and combinational loop\n    * merge statements if it is possible\n    * resolve sensitivitilists\n    * wrap into HWProcess instance\n    * for every IO of process generate name if signal has not any\n    \"\"\"\n    # create copy because this set will be reduced\n    statements = copy(statements)\n\n    # process ranks = how many assignments is probably in process\n    # used to minimize number of merge tries\n    processes = []\n    while statements:\n        stm = statements.pop()\n        proc_statements = [stm, ]\n        ps = _statements_to_HWProcesses(proc_statements, True)\n        processes.extend(ps)\n\n    yield from reduceProcesses(processes)",
    "docstring": "Pack statements into HWProcess instances,\n    * for each out signal resolve it's drivers and collect them\n    * split statements if there is and combinational loop\n    * merge statements if it is possible\n    * resolve sensitivitilists\n    * wrap into HWProcess instance\n    * for every IO of process generate name if signal has not any",
    "model_name": "cohere-command-a-03-2025",
    "summary_english": "The function `statements_to_HWProcesses` transforms a list of HDL statements into a generator of `HWProcess` instances. Its purpose is to organize and optimize the statements by resolving signal drivers, handling combinational loops, merging statements where possible, and generating process names for I/O signals. The function takes a single argument, `statements`, which is a list of `HdlStatement` objects. The key logic involves creating a copy of the input statements, processing them in a loop to form potential processes, and then reducing these processes using the `reduceProcesses` function. Each statement is initially treated as a separate process, and the function `_statements_to_HWProcesses` is used to further process these statements. The final processes are yielded after reduction, ensuring optimization and proper organization.",
    "summary_chinese": "函数名：statements_to_HWProcesses\n\n函数用途：将一组 HDL 语句打包成 HWProcess 实例。\n\n参数：\n- statements（List[HdlStatement]）：一个包含 HDL 语句的列表。\n\n关键逻辑：\n1. 复制输入的语句列表，以便在处理过程中减少原始列表。\n2. 通过循环处理每个语句，将其放入一个临时的过程语句列表中。\n3. 调用辅助函数 _statements_to_HWProcesses 处理这些语句，生成 HWProcess 实例。\n4. 将生成的 HWProcess 实例添加到进程列表中。\n5. 使用 reduceProcesses 函数合并和优化生成的 HWProcess 实例，并通过 yield from 逐个返回最终的 HWProcess 实例。\n\n主要功能包括解决输出信号的驱动程序、处理组合循环、合并语句、解析敏感列表以及为每个过程的 IO 生成名称。",
    "summary_french": "La fonction `statements_to_HWProcesses` a pour but de transformer une liste de déclarations HDL (`HdlStatement`) en instances de `HWProcess`. Elle prend en entrée une liste de déclarations HDL et retourne un générateur d'instances `HWProcess`. \n\nArguments :\n- `statements` (List[HdlStatement]) : une liste de déclarations HDL à transformer.\n\nLogique principale :\nLa fonction commence par créer une copie de la liste de déclarations pour éviter de modifier l'original. Elle initialise ensuite une liste vide `processes` pour stocker les instances `HWProcess` générées. La fonction parcourt ensuite la liste de déclarations, en extrayant chaque déclaration et en la traitant avec la fonction `_statements_to_HWProcesses` pour générer des instances `HWProcess` initiales. Ces instances sont ajoutées à la liste `processes`. Enfin, la fonction utilise `reduceProcesses` pour fusionner et optimiser les instances `HWProcess` générées, et retourne un générateur de ces instances optimisées. La fonction gère également la résolution des pilotes de signaux de sortie, la détection et la gestion des boucles combinatoires, la fusion de déclarations lorsque possible, la résolution des listes de sensibilité, et la génération de noms pour les signaux d'entrée/sortie sans nom.",
    "summary_spanish": "El nombre de la función es `statements_to_HWProcesses`. Su propósito es convertir una lista de declaraciones HDL (HdlStatement) en instancias de HWProcess, realizando varias operaciones como resolver controladores de señales de salida, dividir declaraciones en caso de bucles combinacionales, fusionar declaraciones cuando sea posible, resolver listas de sensibilidad y envolver todo en instancias de HWProcess. Además, genera nombres para las señales de entrada/salida si no los tienen.\n\nArgumentos:\n- `statements` (List[HdlStatement]): Una lista de declaraciones HDL que se procesarán.\n\nLógica clave:\n1. Se crea una copia de la lista de declaraciones para evitar modificar la original.\n2. Se inicializa una lista vacía `processes` para almacenar los procesos resultantes.\n3. Mientras haya declaraciones en la lista, se extrae una declaración y se procesa:\n   - Se crea una lista `proc_statements` con la declaración extraída.\n   - Se llama a la función `_statements_to_HWProcesses` para convertir las declaraciones en procesos HWProcess.\n   - Los procesos resultantes se añaden a la lista `processes`.\n4. Finalmente, se utiliza la función `reduceProcesses` para reducir y optimizar los procesos generados, y se devuelven los resultados mediante `yield from`.",
    "summary_portuguese": "Nome da função: statements_to_HWProcesses\n\nDescrição: Esta função agrupa instruções (statements) em instâncias de HWProcess, realizando várias etapas de processamento, como resolução de drivers, divisão e fusão de instruções, resolução de listas de sensibilidade e geração de nomes para sinais de entrada e saída.\n\nArgumentos:\n- statements (List[HdlStatement]): Uma lista de instruções HDL (Hardware Description Language) a serem processadas.\n\nLógica principal:\nA função começa criando uma cópia da lista de instruções para evitar modificar a lista original. Em seguida, ela processa cada instrução, criando uma lista de instruções para cada processo. A função auxiliar _statements_to_HWProcesses é chamada para converter essas listas em instâncias de HWProcess. Os processos resultantes são armazenados em uma lista e, finalmente, a função reduceProcesses é usada para reduzir e otimizar os processos antes de retorná-los como um gerador. A função também garante que cada sinal de entrada e saída tenha um nome, gerando um se necessário.",
    "summary_arabic": "اسم الدالة: statements_to_HWProcesses\n\nوصف الغرض: تقوم هذه الدالة بتحويل قائمة من بيانات HDL (HdlStatement) إلى مولد (Generator) من كائنات HWProcess. تهدف الدالة إلى تجميع البيانات في عمليات الأجهزة (HWProcess) مع معالجة عدة جوانب مثل حل تعارضات الإشارات، تقسيم أو دمج البيانات عند الحاجة، وحل قوائم الحساسية.\n\nالحجج وأنواعها:\n- statements: قائمة من HdlStatement (List[HdlStatement])\n\nالمنطق الرئيسي:\n1. يتم إنشاء نسخة من قائمة البيانات الأصلية لتجنب تعديلها أثناء المعالجة.\n2. يتم معالجة كل بيان بشكل منفصل، حيث يتم إزالة البيان من القائمة المؤقتة وإنشاء قائمة فرعية تحتوي على هذا البيان فقط.\n3. يتم استدعاء دالة مساعدة (_statements_to_HWProcesses) لتحويل القائمة الفرعية إلى كائنات HWProcess.\n4. يتم جمع جميع كائنات HWProcess الناتجة في قائمة واحدة.\n5. يتم استخدام دالة reduceProcesses لدمج العمليات المتشابهة أو المترابطة.\n6. يتم إرجاع مولد (Generator) من كائنات HWProcess النهائية بعد المعالجة.\n\nملاحظة: الدالة تقوم أيضًا بتوليد أسماء للإشارات التي لا تحتوي على أسماء مسبقة، وضمان معالجة جميع جوانب عمليات الأجهزة بشكل صحيح.",
    "summary_hindi": "फ़ंक्शन का नाम: statements_to_HWProcesses\n\nउद्देश्य: यह फ़ंक्शन HdlStatement की सूची को HWProcess उदाहरणों में पैक करता है। यह प्रत्येक आउटपुट सिग्नल के ड्राइवर्स को हल करता है, कॉम्बिनेशनल लूप की उपस्थिति में स्टेटमेंट्स को विभाजित करता है, संभव होने पर स्टेटमेंट्स को मर्ज करता है, सेंसिटिविटी लिस्ट्स को हल करता है, और उन्हें HWProcess उदाहरण में लपेटता है। साथ ही, प्रत्येक प्रोसेस के IO के लिए नाम जनरेट करता है यदि सिग्नल का कोई नाम नहीं है।\n\nआर्ग्युमेंट्स:\n- statements (List[HdlStatement]): HdlStatement की सूची।\n\nमुख्य तर्क:\n1. स्टेटमेंट्स की एक कॉपी बनाई जाती है क्योंकि यह सेट कम होगा।\n2. प्रोसेस रैंक्स की गणना की जाती है, जो अनुमानित असाइनमेंट्स की संख्या है जो प्रोसेस में हो सकते हैं।\n3. एक लूप में स्टेटमेंट्स को पॉप किया जाता है और उन्हें प्रोसेस स्टेटमेंट्स की सूची में जोड़ा जाता है।\n4"
  }
]